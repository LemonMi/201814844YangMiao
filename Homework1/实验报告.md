#### å®éªŒå†…å®¹

1. é¢„å¤„ç†æ–‡æœ¬æ•°æ®é›†ï¼Œå¹¶ä¸”å¾—åˆ°æ¯ä¸ªæ–‡æœ¬çš„VSMè¡¨ç¤ºã€‚
2. å®ç°KNNåˆ†ç±»å™¨ï¼Œæµ‹è¯•å…¶åœ¨20Newsgroupsä¸Šçš„æ•ˆæœã€‚

#### å®éªŒç¯å¢ƒ

+ Windows 8.1
+ Python 3.6(Anaconda)
+ CPU-only

#### å®éªŒæ­¥éª¤

##### VSMæ¨¡å‹

1. é‡‡ç”¨textblob packageè¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œåˆ†åˆ«å»ºç«‹æ¯ä¸ªæ–‡ä»¶çš„å­—å…¸ï¼Œè®¡ç®—Term Frequency(TF)ï¼Œå¹¶ä»¥å­—å…¸çš„æ–¹å¼å­˜å‚¨åˆ°æœ¬åœ°ã€‚

   + Normalization

   ~~~python
   s = f.read().lower() # All in lower cases
   s = s.replace('/', ' ')
   s = s.replace('-', ' ') # Delete '/' and '-'
   ~~~

   + Tokenization

   ~~~python
   w = TextBlob(s).words
   ~~~

   + Stemming

   ~~~python
   word = Word(word)
   word = word.lemmatize() # deal with noun words
   word = Word(word)
   word = word.lemmatize("v") # deal with verb words
   ~~~

   + Stopwords

   ~~~python
   ntlk.download("stopwords") # download the stopwords dataset
   stop_words = stopwords.words('english')
   clean_w_list = []
   
   if(word not in stop_words and (word not in ['\'s', '\'ll', '\'t'])):
       clean_w_list.append(word)	# delete the words in stopwords
   ~~~

   + ç”Ÿæˆå­—å…¸ï¼Œè®¡ç®—TF

   ~~~python
   clean_w_dict = dict(collections.Counter(clean_w_list))
   ~~~

2. ç”Ÿæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

   - 80% è®­ç»ƒé›†
   - 20% æµ‹è¯•é›†

   æµ‹è¯•é›†å’Œè®­ç»ƒé›†å‡åŒ€çš„ä»æ¯ä¸ªç±»ä¸­éšæœºæŠ½å–ã€‚

3. éå†æ‰€æœ‰è®­ç»ƒé›†æ–‡ä»¶çš„å­—å…¸ï¼Œå»ºç«‹Global Dictionaryï¼Œå¹¶è®¡ç®—Document Frequency(DF)ã€‚è¿‡æ»¤æ‰DFå°äº15çš„å•è¯åç”Ÿæˆæ–°çš„å­—å…¸ã€‚

   ~~~python
   global_dict={}
   for key in tmp_file_dict:
       if(key in global_dict.keys()):
           global_dict[key] += 1
       else:
           global_dict[key] = 1
   ~~~

4. æ ¹æ®æ­¥éª¤1ä¸­è®¡ç®—çš„TFå’Œæ­¥éª¤3ä¸­è®¡ç®—çš„DFï¼Œè®¡ç®—TF-IDF Weightï¼Œå»ºç«‹Vectorã€‚
   $$
   Weight=TF*IDF
   $$

   + Term Frequency(TF) with sub-linear scaling:
     $$
     TF(t,d)=
     \begin{equation}
     \left\{
     \begin{array}{lr}
     1+log(c(t,d)),&c(t,d)>0\\
     0,&otherwise
     \end{array}
     \right.
     \end{equation}\\
     ğ‘(ğ‘¡,ğ‘‘)\;be\;the\;frequency\;count\;of\;term\;ğ‘¡\;in\;doc\;ğ‘‘
     $$

   + Inverse Document Frequency(IDF) with non-linear scaling:
     $$
     IDF(t)=log(\frac{N}{df(t)})\\
     df(t)\;Number\;of\;docs\;containing\;term\;ğ‘¡\\
     N\;Total\;number\;of\;docs\;in\;collection
     $$




   é¦–å…ˆå°†å­—å…¸ä¸­çš„æ¯ä¸€ä¸ªtermæ˜ å°„åˆ°indexä¸Šï¼Œç„¶åéå†æ¯ä¸ªæ–‡ä»¶çš„æ¯ä¸ªtermï¼Œå¹¶è®¡ç®—æ¯ä¸ªtermçš„weightæ”¾åˆ°å¯¹åº”çš„indexä¸‹æ¥æ„é€ vectorã€‚ï¼ˆè€ƒè™‘åˆ°å­—å…¸çš„é•¿åº¦ä»¥åŠæ–‡ä»¶çš„ä¸ªæ•°è¿‡å¤§ï¼Œä¸ºæå‡æ•ˆç‡ï¼Œé‡‡ç”¨scipy.sparseåŒ…ä¸­çš„csr_matrixç¨€ç–çŸ©é˜µæ¥å­˜å‚¨ã€‚ï¼‰

##### KNNæ¨¡å‹

###### KNNåŸç†ï¼š

è®­ç»ƒé›†æ˜¯mä¸ªn-dimensionçš„vectorï¼Œå¹¶ä¸”å·²çŸ¥ç›¸åº”çš„labelã€‚å¯¹æµ‹è¯•é›†ä¸­çš„æ¯ä¸ªæ•°æ®ï¼Œè®¡ç®—è®­ç»ƒé›†ä¸­è·ç¦»å…¶æœ€è¿‘çš„kä¸ªneighborsï¼Œå…¶ä¸­æŠ•ç¥¨æœ€å¤šlabelå³ä¸ºæµ‹è¯•é›†ä¸­è¯¥æ•°æ®çš„labelã€‚

###### è·ç¦»çš„å®šä¹‰ï¼š

+ Euclidean distance 
  $$
  d(p,q)=\sqrt{\sum_{i=1}(p_i-q_i)^2}
  $$

+ Cosine Similarity
  $$
  d(p,q)=-cosine(p,q)=-\frac{{V_p}^TV_q}{|V_P|_2|V_q|_2}
  $$


åœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œé€‰æ‹©çš„æ˜¯Cosine Similarity

###### Voteçš„æƒé‡ï¼š



#### å®éªŒç»“æœ

