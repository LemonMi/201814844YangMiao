#### 实验内容

1. 预处理文本数据集，并且得到每个文本的VSM表示。
2. 实现KNN分类器，测试其在20Newsgroups上的效果。

#### 实验环境

+ Windows 8.1
+ Python 3.6(Anaconda)
+ CPU-only

#### 实验步骤

##### VSM模型

1. 采用textblob package进行数据预处理，分别建立每个文件的字典，计算Term Frequency(TF)，并以字典的方式存储到本地。

   + Normalization

   ~~~python
   s = f.read().lower() # All in lower cases
   s = s.replace('/', ' ')
   s = s.replace('-', ' ') # Delete '/' and '-'
   ~~~

   + Tokenization

   ~~~python
   w = TextBlob(s).words
   ~~~

   + Stemming

   ~~~python
   word = Word(word)
   word = word.lemmatize() # deal with noun words
   word = Word(word)
   word = word.lemmatize("v") # deal with verb words
   ~~~

   + Stopwords

   ~~~python
   ntlk.download("stopwords") # download the stopwords dataset
   stop_words = stopwords.words('english')
   clean_w_list = []
   
   if(word not in stop_words and (word not in ['\'s', '\'ll', '\'t'])):
       clean_w_list.append(word)	# delete the words in stopwords
   ~~~

   + 生成字典，计算TF

   ~~~python
   clean_w_dict = dict(collections.Counter(clean_w_list))
   ~~~

2. 生成训练集和测试集。

   - 80% 训练集
   - 20% 测试集

   测试集和训练集均匀的从每个类中随机抽取。

3. 遍历所有训练集文件的字典，建立Global Dictionary，计算Document Frequency(DF)。过滤掉DF小于15的单词后生成新的字典。

   ~~~python
   global_dict={}
   for key in tmp_file_dict:
       if(key in global_dict.keys()):
           global_dict[key] += 1
       else:
           global_dict[key] = 1
   ~~~

4. 根据步骤1中计算的TF和步骤3中计算的DF，计算TF-IDF Weight，建立Vector。
   
      <img src="http://latex.codecogs.com/gif.latex?Weight=TF*IDF" />

   + Term Frequency(TF) with sub-linear scaling:
      
      <img src="http://latex.codecogs.com/gif.latex?TF(t,d)=1+log(c(t,d))\\c(t,d)\;be\;the\;frequency\;count\;of\;term\;t\;in\;doc\;d" />

   + Inverse Document Frequency(IDF) with non-linear scaling:
      
      <img src="http://latex.codecogs.com/gif.latex?IDF(t)=log(\frac{N}{df(t)})\\df(t)\;Number\;of\;docs\;containing\;term\;𝑡\\N\;Total\;number\;of\;docs\;in\;collection" />

首先将字典中的每一个term映射到index上，然后遍历每个文件的每个term，并计算每个term的weight放到对应的index下来构造vector。（考虑到字典的长度以及文件的个数过大，为提升效率，采用scipy.sparse包中的csr_matrix稀疏矩阵来存储。）

##### KNN模型

###### KNN原理：

训练集是m个n-dimension的vector，并且已知相应的label。对测试集中的每个数据，计算训练集中距离其最近的k个neighbors（本实验中选取的数值是5），其中投票最多label即为测试集中该数据的label。

###### 距离的定义：

+ Euclidean distance 
   
   <img src="http://latex.codecogs.com/gif.latex?d(p,q)=\sqrt{\sum_{i=1}(p_i-q_i)^2}" />

+ Cosine Similarity
   
   <img src="http://latex.codecogs.com/gif.latex?d(p,q)=-cosine(p,q)=-\frac{{V_p}^TV_q}{|V_P|_2|V_q|_2}" />

在本次实验中，选择的是Cosine Similarity作为衡量距离的单位。

###### Vote的权重：

根据距离给k个neighbors的vote赋予不同的权重。
   
   <img src="http://latex.codecogs.com/gif.latex?weight\;factor=\frac{1}{d^2}" />

赋予权重的意义：

+ 可以使得投票结果不仅取决于票数，且同时更趋向于距离更近的点。
+ 当存在两个票数相同的label的时候，会选择距离整体距离更近的label，增加准确率。

###### 提高算法的效率：

在计算KNN时，需要计算测试集中每个vector和训练集中每个vector的距离，时间复杂度至少为o(10^7)

参考YangLin的思路，选择了sklearn.neighbors的BallTree来提高算法效率。由于BallTree的内置距离不包含Cosine距离，可用向量归一化后的欧氏距离代替Cosine距离。推导如下：

<img src="http://latex.codecogs.com/gif.latex?Euclidean\;Distance=\frac{(X-Y)(X-Y)^T}{|X|_2|Y|_2}" />

将上式展开可得：<img src="http://latex.codecogs.com/gif.latex?ED=XX^T+YY^T-XY^T-YX^T" />

把X、Y向量归一化后，上式可简化为：<img src="http://latex.codecogs.com/gif.latex?ED=\frac{2-2XY^T}{|X|_2|Y|_2}" />

由上式可知，向量归一化后的欧氏距离和Cosine距离是成反比的。且向量归一化后，向量之间的夹角是不变的。

#### 文件结构

+ 主目录

| 20news-18828   存放文件数据

| file_dict_folder   存放每个文件的字典

| result_balltree   存放BallTree的运行结果

| result_original   存放原始方法的运行结果

**| VSM_KNN.py 运行代码**

+ 结果目录（result_balltree/result_original）

|| global_dict_filtered.txt   训练集字典

|| output.txt   输出的log文件（未知原因log后缀不能与github同步，因此修改为txt文件）

|| testing_path.txt   保存测试集路径

|| training_path.txt   保存训练集路径

#### 实验结果

BallTree Result:(with vote weight)

>true_num:3318 total_num:3759 accuracy:0.882682
>The Ball Tree Query has finished: it takes 962.904263s

Original Result:(without vote weight)

>true_num:3265 total_num:3759 accuracy:0.868582
>The KNN has finished: it takes 19946.281052s

由于Cosine Similarity越大越好，即d值越大两个vector越接近。而vote的weight与d成反比，d越大weight反而越小，与事实冲突。应该是d值越大weight越大才对。因而考虑在原始方法中不使用Vote Weight。

在BallTree方法中，由于最后计算的是欧式距离，所以满足Vote Weight的定义，可以使用。由结果可知，其可以在一定程度上提升预测的准确率。
