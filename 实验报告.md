#### 实验内容

1. 预处理文本数据集，并且得到每个文本的VSM表示。
2. 实现KNN分类器，测试其在20Newsgroups上的效果。

#### 实验环境

+ Windows 8.1
+ Python 3.6(Anaconda)
+ CPU-only

#### 实验步骤

##### VSM模型

1. 采用textblob package进行数据预处理，分别建立每个文件的字典，计算Term Frequency(TF)，并以字典的方式存储到本地。

   + Normalization

   ~~~python
   s = f.read().lower() # All in lower cases
   s = s.replace('/', ' ')
   s = s.replace('-', ' ') # Delete '/' and '-'
   ~~~

   + Tokenization

   ~~~python
   w = TextBlob(s).words
   ~~~

   + Stemming

   ~~~python
   word = Word(word)
   word = word.lemmatize() # deal with noun words
   word = Word(word)
   word = word.lemmatize("v") # deal with verb words
   ~~~

   + Stopwords

   ~~~python
   ntlk.download("stopwords") # download the stopwords dataset
   stop_words = stopwords.words('english')
   clean_w_list = []
   
   if(word not in stop_words and (word not in ['\'s', '\'ll', '\'t'])):
       clean_w_list.append(word)	# delete the words in stopwords
   ~~~

   + 生成字典，计算TF

   ~~~python
   clean_w_dict = dict(collections.Counter(clean_w_list))
   ~~~

2. 生成训练集和测试集。

   - 80% 训练集
   - 20% 测试集

   测试集和训练集均匀的从每个类中随机抽取。

3. 遍历所有训练集文件的字典，建立Global Dictionary，并计算Document Frequency(DF)。过滤掉DF小于15的单词后生成新的字典。

   ~~~python
   global_dict={}
   for key in tmp_file_dict:
       if(key in global_dict.keys()):
           global_dict[key] += 1
       else:
           global_dict[key] = 1
   ~~~

4. 根据步骤1中计算的TF和步骤3中计算的DF，计算TF-IDF Weight，建立Vector。
   $$
   Weight=TF*IDF
   $$

   + Term Frequency(TF) with sub-linear scaling:
     $$
     TF(t,d)=
     \begin{equation}
     \left\{
     \begin{array}{lr}
     1+log(c(t,d)),&c(t,d)>0\\
     0,&otherwise
     \end{array}
     \right.
     \end{equation}\\
     𝑐(𝑡,𝑑)\;be\;the\;frequency\;count\;of\;term\;𝑡\;in\;doc\;𝑑
     $$

   + Inverse Document Frequency(IDF) with non-linear scaling:
     $$
     IDF(t)=log(\frac{N}{df(t)})\\
     df(t)\;Number\;of\;docs\;containing\;term\;𝑡\\
     N\;Total\;number\;of\;docs\;in\;collection
     $$




   首先将字典中的每一个term映射到index上，然后遍历每个文件的每个term，并计算每个term的weight放到对应的index下来构造vector。（考虑到字典的长度以及文件的个数过大，为提升效率，采用scipy.sparse包中的csr_matrix稀疏矩阵来存储。）

##### KNN模型

###### KNN原理：

训练集是m个n-dimension的vector，并且已知相应的label。对测试集中的每个数据，计算训练集中距离其最近的k个neighbors，其中投票最多label即为测试集中该数据的label。

###### 距离的定义：

+ Euclidean distance 
  $$
  d(p,q)=\sqrt{\sum_{i=1}(p_i-q_i)^2}
  $$

+ Cosine Similarity
  $$
  d(p,q)=-cosine(p,q)=-\frac{{V_p}^TV_q}{|V_P|_2|V_q|_2}
  $$


在本次实验中，选择的是Cosine Similarity

###### Vote的权重：



#### 实验结果

